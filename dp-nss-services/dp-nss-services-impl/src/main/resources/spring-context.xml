<beans xmlns="http://www.springframework.org/schema/beans"
       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
       xmlns:context="http://www.springframework.org/schema/context"
       xmlns:tx="http://www.springframework.org/schema/tx"
       xmlns:batch="http://www.springframework.org/schema/batch"
       xsi:schemaLocation="http://www.springframework.org/schema/beans
        http://www.springframework.org/schema/beans/spring-beans.xsd
        http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd
        http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd
        http://www.springframework.org/schema/batch http://www.springframework.org/schema/batch/spring-batch-3.0.xsd">

    <context:component-scan base-package="cz.cvut.dp.nss.services"/>
    <context:component-scan base-package="cz.cvut.dp.nss.persistence"/>
    <context:component-scan base-package="cz.cvut.dp.nss.context"/>
    <context:component-scan base-package="cz.cvut.dp.nss.batch"/>

    <context:component-scan base-package="cz.cvut.dp.nss.graph"/>

    <tx:annotation-driven transaction-manager="transactionManager" order="0"/>

    <context:property-placeholder location="classpath:dp-nssLocal.properties" />

    <!-- security -->
    <bean id="passwordEncoder" class="org.springframework.security.crypto.bcrypt.BCryptPasswordEncoder">
        <constructor-arg name="strength" value="11" />
    </bean>

    <!-- database -->
    <bean id="dataSource" class="org.apache.commons.dbcp.BasicDataSource" destroy-method="close">
        <property name="driverClassName" value="${jdbc.driver}" />
        <property name="url" value="${jdbc.url}" />
        <property name="username" value="${jdbc.username}" />
        <property name="password" value="${jdbc.password}" />
        <property name="maxIdle" value="10" />
        <property name="maxActive" value="20" />
        <property name="poolPreparedStatements" value="true" />
    </bean>

    <bean id="sessionFactory" class="org.springframework.orm.hibernate5.LocalSessionFactoryBean">
        <property name="dataSource" ref="dataSource" />
        <property name="packagesToScan" value="cz.cvut.dp.nss.services"/>

        <property name="hibernateProperties">
            <map>
                <entry key="hibernate.dialect" value="${hibernate.dialect}"/>
                <entry key="hibernate.show_sql" value="${hibernate.showSql}"/>
                <entry key="hibernate.format_sql" value="${hibernate.formatSql}"/>
                <entry key="hibernate.hbm2ddl.auto" value="${hibernate.auto}"/>
                <entry key="hibernate.current_session_context_class" value="org.springframework.orm.hibernate5.SpringSessionContext"/>

                <entry key="hibernate.multiTenancy" value="SCHEMA"/>
                <entry key="hibernate.tenant_identifier_resolver" value-ref="currentTenantResolverImpl"/>
                <entry key="hibernate.multi_tenant_connection_provider" value-ref="schemaPerTenantConnectionProvider"/>
            </map>
        </property>
    </bean>

    <bean id="transactionManager" class="org.springframework.orm.hibernate5.HibernateTransactionManager">
        <property name="autodetectDataSource" value="false"/>
        <property name="sessionFactory" ref="sessionFactory" />
    </bean>

    <!-- GRAPH CONTEXT -->
    <import resource="classpath:spring-graph-context.xml"/>


    <!-- SPRING JDBC -->
    <bean class="org.springframework.jdbc.core.namedparam.NamedParameterJdbcTemplate" id="namedParameterJdbcTemplate" >
        <constructor-arg type="javax.sql.DataSource" ref="dataSource"/>
    </bean>

    <bean id="jdbcSearchDao" class="cz.cvut.dp.nss.persistence.trip.JdbcTripDao">
        <property name="dataSource" ref="dataSource" />
        <property name="namedParameterJdbcTemplate" ref="namedParameterJdbcTemplate" />
    </bean>




    <!-- SPRING BATCH -->
    <bean id="jobRepository" class="org.springframework.batch.core.repository.support.MapJobRepositoryFactoryBean">
        <property name="transactionManager" ref="transactionManager" />
        <!-- toto je velmi dulezite, default by (u postgre) mel byt READ_COMMITED coz je dle dokumentace OK -->
        <!-- viz http://docs.spring.io/spring-batch/reference/html/configureJob.html#txConfigForJobRepository -->
        <property name="isolationLevelForCreate" value="ISOLATION_DEFAULT"/>
    </bean>

    <bean id="jobLauncher" class="org.springframework.batch.core.launch.support.SimpleJobLauncher">
        <property name="jobRepository" ref="jobRepository" />
    </bean>


    <!-- EXPORTS -->
    <bean id="routeExportReader" class="org.springframework.batch.item.database.JdbcPagingItemReader" scope="step">
        <property name="dataSource" ref="dataSource" />
        <property name="queryProvider">
            <bean class="org.springframework.batch.item.database.support.SqlPagingQueryProviderFactoryBean">
                <property name="dataSource" ref="dataSource" />
                <property name="selectClause" value="select id,agency_id,shortName,longName,routeType,color" />
                <property name="fromClause" value="from #{jobParameters['schema']}.routes" />
                <property name="sortKey" value="id" />
            </bean>
        </property>
        <property name="pageSize" value="100" />
        <property name="rowMapper">
            <bean class="cz.cvut.dp.nss.batch.output.route.RouteBatchRowMapper" />
        </property>
    </bean>

    <bean id="routeExportWriter" class="cz.cvut.dp.nss.batch.output.route.RouteBatchExportWriter" scope="step">
        <!-- write to this csv file -->
        <property name="resource" value="file:#{jobParameters['exportFileLocation']}/routes.txt" />
        <property name="shouldDeleteIfExists" value="true" />

        <property name="lineAggregator">
            <bean class="org.springframework.batch.item.file.transform.DelimitedLineAggregator">
                <property name="delimiter" value="," />
                <property name="fieldExtractor">
                    <bean class="org.springframework.batch.item.file.transform.BeanWrapperFieldExtractor">
                        <property name="names" value="id,agencyId,shortName,longName,routeType,color" />
                    </bean>
                </property>
            </bean>
        </property>
    </bean>


    <bean id="agencyExportReader" class="org.springframework.batch.item.database.JdbcPagingItemReader" scope="step">
        <property name="dataSource" ref="dataSource" />
        <property name="queryProvider">
            <bean class="org.springframework.batch.item.database.support.SqlPagingQueryProviderFactoryBean">
                <property name="dataSource" ref="dataSource" />
                <property name="selectClause" value="select id,name,url,phone" />
                <property name="fromClause" value="from #{jobParameters['schema']}.agencies" />
                <property name="sortKey" value="id" />
            </bean>
        </property>
        <property name="pageSize" value="100" />
        <property name="rowMapper">
            <bean class="cz.cvut.dp.nss.batch.output.agency.AgencyBatchRowMapper" />
        </property>
    </bean>

    <bean id="agencyExportWriter" class="cz.cvut.dp.nss.batch.output.agency.AgencyBatchExportWriter" scope="step">
        <!-- write to this csv file -->
        <property name="resource" value="file:#{jobParameters['exportFileLocation']}/agency.txt" />
        <property name="shouldDeleteIfExists" value="true" />

        <property name="lineAggregator">
            <bean class="org.springframework.batch.item.file.transform.DelimitedLineAggregator">
                <property name="delimiter" value="," />
                <property name="fieldExtractor">
                    <bean class="org.springframework.batch.item.file.transform.BeanWrapperFieldExtractor">
                        <property name="names" value="id,name,url,phone" />
                    </bean>
                </property>
            </bean>
        </property>
    </bean>


    <bean id="calendarExportReader" class="org.springframework.batch.item.database.JdbcPagingItemReader" scope="step">
        <property name="dataSource" ref="dataSource" />
        <property name="queryProvider">
            <bean class="org.springframework.batch.item.database.support.SqlPagingQueryProviderFactoryBean">
                <property name="dataSource" ref="dataSource" />
                <property name="selectClause" value="select id,monday,tuesday,wednesday,thursday,friday,saturday,sunday,startDate,endDate" />
                <property name="fromClause" value="from #{jobParameters['schema']}.calendar" />
                <property name="sortKey" value="id" />
            </bean>
        </property>
        <property name="pageSize" value="100" />
        <property name="rowMapper">
            <bean class="cz.cvut.dp.nss.batch.output.calendar.CalendarBatchRowMapper" />
        </property>
    </bean>

    <bean id="calendarExportWriter" class="cz.cvut.dp.nss.batch.output.calendar.CalendarBatchExportWriter" scope="step">
        <!-- write to this csv file -->
        <property name="resource" value="file:#{jobParameters['exportFileLocation']}/calendar.txt" />
        <property name="shouldDeleteIfExists" value="true" />

        <property name="lineAggregator">
            <bean class="org.springframework.batch.item.file.transform.DelimitedLineAggregator">
                <property name="delimiter" value="," />
                <property name="fieldExtractor">
                    <bean class="org.springframework.batch.item.file.transform.BeanWrapperFieldExtractor">
                        <property name="names" value="id,monday,tuesday,wednesday,thursday,friday,saturday,sunday,startDate,endDate" />
                    </bean>
                </property>
            </bean>
        </property>
    </bean>


    <batch:job id="gtfsExportRouteBatchJob">
        <batch:step id="routeExportStep1">
            <batch:tasklet>
                <batch:chunk reader="routeExportReader" writer="routeExportWriter" commit-interval="100">
                </batch:chunk>
            </batch:tasklet>
        </batch:step>
    </batch:job>

    <batch:job id="gtfsExportAgencyBatchJob">
        <batch:step id="agencyExportStep1">
            <batch:tasklet>
                <batch:chunk reader="agencyExportReader" writer="agencyExportWriter" commit-interval="100">
                </batch:chunk>
            </batch:tasklet>
        </batch:step>
    </batch:job>

    <batch:job id="gtfsExportCalendarBatchJob">
        <batch:step id="calendarExportStep1">
            <batch:tasklet>
                <batch:chunk reader="calendarExportReader" writer="calendarExportWriter" commit-interval="100">
                </batch:chunk>
            </batch:tasklet>
        </batch:step>
    </batch:job>








    <!-- IMPORTS -->
    <!-- zpracovani zahlavi csv souboru -->
    <bean id="batchHeaderLineDynamicCallbackHandler" class="cz.cvut.dp.nss.batch.input.BatchHeaderLineDynamicCallbackHandler" scope="step"/>

    <!-- namapovani sloupcu dle jejich jmen ze zahlavi a kompletni cteni csv souboru -->
    <bean id="batchLineDynamicTokenizer" class="cz.cvut.dp.nss.batch.input.BatchLineDynamicTokenizer" scope="step">
        <property name="delimiter" value=","/>
    </bean>

    <batch:job id="gtfsImportAgencyBatchJob">
        <batch:step id="agencyStep1">
            <batch:tasklet>
                <batch:chunk reader="agencyBatchReader" processor="agencyBatchProcessor" writer="agencyBatchWriter" commit-interval="10" />
            </batch:tasklet>
            <batch:listeners>
                <batch:listener ref="batchHeaderLineDynamicCallbackHandler"/>
                <batch:listener ref="batchLineDynamicTokenizer"/>
            </batch:listeners>
        </batch:step>
    </batch:job>

    <batch:job id="gtfsImportRouteBatchJob">
        <batch:step id="routeStep1">
            <batch:tasklet>
                <batch:chunk reader="routeBatchReader" processor="routeBatchProcessor" writer="routeBatchWriter" commit-interval="50" />
            </batch:tasklet>
            <batch:listeners>
                <batch:listener ref="batchHeaderLineDynamicCallbackHandler"/>
                <batch:listener ref="batchLineDynamicTokenizer"/>
            </batch:listeners>
        </batch:step>
    </batch:job>

    <batch:job id="gtfsImportCalendarBatchJob">
        <batch:step id="calendarStep1">
            <batch:tasklet>
                <batch:chunk reader="calendarBatchReader" processor="calendarBatchProcessor" writer="calendarBatchWriter" commit-interval="10" />
            </batch:tasklet>
            <batch:listeners>
                <batch:listener ref="batchHeaderLineDynamicCallbackHandler"/>
                <batch:listener ref="batchLineDynamicTokenizer"/>
            </batch:listeners>
        </batch:step>
    </batch:job>

    <batch:job id="gtfsImportCalendarDateBatchJob">
        <batch:step id="calendarDateStep1">
            <batch:tasklet>
                <batch:chunk reader="calendarDateBatchReader" processor="calendarDateBatchProcessor" writer="calendarDateBatchWriter" commit-interval="50" />
            </batch:tasklet>
            <batch:listeners>
                <batch:listener ref="batchHeaderLineDynamicCallbackHandler"/>
                <batch:listener ref="batchLineDynamicTokenizer"/>
            </batch:listeners>
        </batch:step>
    </batch:job>

    <batch:job id="gtfsImportStopParentBatchJob">
        <batch:step id="stopParentStep1">
            <batch:tasklet>
                <batch:chunk reader="stopBatchReader" processor="stopParentBatchProcessor" writer="stopBatchWriter" commit-interval="100" />
            </batch:tasklet>
            <batch:listeners>
                <batch:listener ref="batchHeaderLineDynamicCallbackHandler"/>
                <batch:listener ref="batchLineDynamicTokenizer"/>
            </batch:listeners>
        </batch:step>
    </batch:job>

    <batch:job id="gtfsImportStopChildBatchJob">
        <batch:step id="stopChildStep1">
            <batch:tasklet>
                <batch:chunk reader="stopBatchReader" processor="stopChildBatchProcessor" writer="stopBatchWriter" commit-interval="100" />
            </batch:tasklet>
            <batch:listeners>
                <batch:listener ref="batchHeaderLineDynamicCallbackHandler"/>
                <batch:listener ref="batchLineDynamicTokenizer"/>
            </batch:listeners>
        </batch:step>
    </batch:job>

    <batch:job id="gtfsImportShapeBatchJob">
        <batch:step id="shapeStep1">
            <batch:tasklet>
                <batch:chunk reader="shapeBatchReader" processor="shapeBatchProcessor" writer="shapeBatchWriter" commit-interval="150" />
            </batch:tasklet>
            <batch:listeners>
                <batch:listener ref="batchHeaderLineDynamicCallbackHandler"/>
                <batch:listener ref="batchLineDynamicTokenizer"/>
            </batch:listeners>
        </batch:step>
    </batch:job>

    <batch:job id="gtfsImportTripBatchJob">
        <batch:step id="tripStep1">
            <batch:tasklet>
                <batch:chunk reader="tripBatchReader" processor="tripBatchProcessor" writer="tripBatchWriter" commit-interval="150" />
            </batch:tasklet>
            <batch:listeners>
                <batch:listener ref="batchHeaderLineDynamicCallbackHandler"/>
                <batch:listener ref="batchLineDynamicTokenizer"/>
            </batch:listeners>
        </batch:step>
    </batch:job>

    <batch:job id="gtfsImportStopTimeBatchJob">
        <batch:step id="stopTimeStep1">
            <batch:tasklet>
                <batch:chunk reader="stopTimeBatchReader" processor="stopTimeBatchProcessor" writer="stopTimeBatchWriter" commit-interval="150" />
            </batch:tasklet>
            <batch:listeners>
                <batch:listener ref="batchHeaderLineDynamicCallbackHandler"/>
                <batch:listener ref="batchLineDynamicTokenizer"/>
            </batch:listeners>
        </batch:step>
    </batch:job>

    <batch:job id="graphImportTripBatchJob">
        <batch:step id="graphImportTripStep1">
            <batch:tasklet>
                <batch:chunk reader="graphTripBatchReader" processor="graphTripBatchProcessor" writer="graphTripBatchWriter" commit-interval="6" />
            </batch:tasklet>
        </batch:step>
    </batch:job>

    <batch:job id="graphConnectStopBatchJob">
        <batch:step id="graphConnectStopStep1">
            <batch:tasklet>
                <batch:chunk reader="graphStopBatchReader" processor="graphStopBatchProcessor" writer="graphStopBatchWriter" commit-interval="1" />
            </batch:tasklet>
        </batch:step>
    </batch:job>

    <batch:job id="graphImportCalendarBatchJob">
        <batch:step id="graphImportCalendarStep1">
            <batch:tasklet>
                <batch:chunk reader="graphCalendarBatchReader" processor="graphCalendarBatchProcessor" writer="graphCalendarBatchWriter" commit-interval="20" />
            </batch:tasklet>
        </batch:step>
    </batch:job>

    <bean id="agencyBatchReader" class="cz.cvut.dp.nss.batch.input.agency.AgencyBatchReader" scope="step">

        <!-- csv soubor k importu -->
        <property name="resource" value="#{jobParameters['agencyLocation']}" />

        <!-- preskoc zahlavi -->
        <property name="linesToSkip" value="1"/>

        <!-- ale zpracuj ho pomoci callbacku -->
        <property name="skippedLinesCallback" ref="batchHeaderLineDynamicCallbackHandler"/>

        <!-- mapovani radku csv-->
        <property name="lineMapper">
            <bean class="org.springframework.batch.item.file.mapping.DefaultLineMapper">
                <!-- rozseknuti hodnot radku-->
                <property name="lineTokenizer" ref="batchLineDynamicTokenizer"/>
                <!-- do processoru budeme posilat DefaultFieldSet kde se budeme do mapy dotazovat na property -->
                <property name="fieldSetMapper">
                    <bean class="org.springframework.batch.item.file.mapping.PassThroughFieldSetMapper" />
                </property>
            </bean>
        </property>
    </bean>

    <bean id="routeBatchReader" class="cz.cvut.dp.nss.batch.input.route.RouteBatchReader" scope="step">

        <!-- csv soubor k importu -->
        <property name="resource" value="#{jobParameters['routesLocation']}" />

        <!-- preskoc zahlavi -->
        <property name="linesToSkip" value="1"/>

        <!-- ale zpracuj ho pomoci callbacku -->
        <property name="skippedLinesCallback" ref="batchHeaderLineDynamicCallbackHandler"/>

        <!-- mapovani radku csv-->
        <property name="lineMapper">
            <bean class="org.springframework.batch.item.file.mapping.DefaultLineMapper">
                <!-- rozseknuti hodnot radku-->
                <property name="lineTokenizer" ref="batchLineDynamicTokenizer"/>
                <!-- do processoru budeme posilat DefaultFieldSet kde se budeme do mapy dotazovat na property -->
                <property name="fieldSetMapper">
                    <bean class="org.springframework.batch.item.file.mapping.PassThroughFieldSetMapper" />
                </property>
            </bean>
        </property>
    </bean>

    <bean id="calendarBatchReader" class="cz.cvut.dp.nss.batch.input.calendar.CalendarBatchReader" scope="step">

        <!-- csv soubor k importu -->
        <property name="resource" value="#{jobParameters['calendarLocation']}" />

        <!-- preskoc zahlavi -->
        <property name="linesToSkip" value="1"/>

        <!-- ale zpracuj ho pomoci callbacku -->
        <property name="skippedLinesCallback" ref="batchHeaderLineDynamicCallbackHandler"/>

        <!-- mapovani radku csv-->
        <property name="lineMapper">
            <bean class="org.springframework.batch.item.file.mapping.DefaultLineMapper">
                <!-- rozseknuti hodnot radku-->
                <property name="lineTokenizer" ref="batchLineDynamicTokenizer"/>
                <!-- do processoru budeme posilat DefaultFieldSet kde se budeme do mapy dotazovat na property -->
                <property name="fieldSetMapper">
                    <bean class="org.springframework.batch.item.file.mapping.PassThroughFieldSetMapper" />
                </property>
            </bean>
        </property>
    </bean>

    <bean id="calendarDateBatchReader" class="cz.cvut.dp.nss.batch.input.calendarDate.CalendarDateBatchReader" scope="step">

        <!-- csv soubor k importu -->
        <property name="resource" value="#{jobParameters['calendarDateLocation']}" />

        <!-- preskoc zahlavi -->
        <property name="linesToSkip" value="1"/>

        <!-- ale zpracuj ho pomoci callbacku -->
        <property name="skippedLinesCallback" ref="batchHeaderLineDynamicCallbackHandler"/>

        <!-- mapovani radku csv-->
        <property name="lineMapper">
            <bean class="org.springframework.batch.item.file.mapping.DefaultLineMapper">
                <!-- rozseknuti hodnot radku-->
                <property name="lineTokenizer" ref="batchLineDynamicTokenizer"/>
                <!-- do processoru budeme posilat DefaultFieldSet kde se budeme do mapy dotazovat na property -->
                <property name="fieldSetMapper">
                    <bean class="org.springframework.batch.item.file.mapping.PassThroughFieldSetMapper" />
                </property>
            </bean>
        </property>
    </bean>

    <bean id="shapeBatchReader" class="cz.cvut.dp.nss.batch.input.shape.ShapeBatchReader" scope="step">

        <!-- csv soubor k importu -->
        <property name="resource" value="#{jobParameters['shapesLocation']}" />

        <!-- preskoc zahlavi -->
        <property name="linesToSkip" value="1"/>

        <!-- ale zpracuj ho pomoci callbacku -->
        <property name="skippedLinesCallback" ref="batchHeaderLineDynamicCallbackHandler"/>

        <!-- mapovani radku csv-->
        <property name="lineMapper">
            <bean class="org.springframework.batch.item.file.mapping.DefaultLineMapper">
                <!-- rozseknuti hodnot radku-->
                <property name="lineTokenizer" ref="batchLineDynamicTokenizer"/>
                <!-- do processoru budeme posilat DefaultFieldSet kde se budeme do mapy dotazovat na property -->
                <property name="fieldSetMapper">
                    <bean class="org.springframework.batch.item.file.mapping.PassThroughFieldSetMapper" />
                </property>
            </bean>
        </property>
    </bean>

    <bean id="stopBatchReader" class="cz.cvut.dp.nss.batch.input.stop.StopBatchReader" scope="step">

        <!-- csv soubor k importu -->
        <property name="resource" value="#{jobParameters['stopsLocation']}" />

        <!-- preskoc zahlavi -->
        <property name="linesToSkip" value="1"/>

        <!-- ale zpracuj ho pomoci callbacku -->
        <property name="skippedLinesCallback" ref="batchHeaderLineDynamicCallbackHandler"/>

        <!-- mapovani radku csv-->
        <property name="lineMapper">
            <bean class="org.springframework.batch.item.file.mapping.DefaultLineMapper">
                <!-- rozseknuti hodnot radku-->
                <property name="lineTokenizer" ref="batchLineDynamicTokenizer"/>
                <!-- do processoru budeme posilat DefaultFieldSet kde se budeme do mapy dotazovat na property -->
                <property name="fieldSetMapper">
                    <bean class="org.springframework.batch.item.file.mapping.PassThroughFieldSetMapper" />
                </property>
            </bean>
        </property>
    </bean>

    <bean id="tripBatchReader" class="cz.cvut.dp.nss.batch.input.trip.TripBatchReader" scope="step">

        <!-- csv soubor k importu -->
        <property name="resource" value="#{jobParameters['tripsLocation']}" />

        <!-- preskoc zahlavi -->
        <property name="linesToSkip" value="1"/>

        <!-- ale zpracuj ho pomoci callbacku -->
        <property name="skippedLinesCallback" ref="batchHeaderLineDynamicCallbackHandler"/>

        <!-- mapovani radku csv-->
        <property name="lineMapper">
            <bean class="org.springframework.batch.item.file.mapping.DefaultLineMapper">
                <!-- rozseknuti hodnot radku-->
                <property name="lineTokenizer" ref="batchLineDynamicTokenizer"/>
                <!-- do processoru budeme posilat DefaultFieldSet kde se budeme do mapy dotazovat na property -->
                <property name="fieldSetMapper">
                    <bean class="org.springframework.batch.item.file.mapping.PassThroughFieldSetMapper" />
                </property>
            </bean>
        </property>
    </bean>

    <bean id="stopTimeBatchReader" class="cz.cvut.dp.nss.batch.input.stopTime.StopTimeBatchReader" scope="step">

        <!-- csv soubor k importu -->
        <property name="resource" value="#{jobParameters['stopTimesLocation']}" />

        <!-- preskoc zahlavi -->
        <property name="linesToSkip" value="1"/>

        <!-- ale zpracuj ho pomoci callbacku -->
        <property name="skippedLinesCallback" ref="batchHeaderLineDynamicCallbackHandler"/>

        <!-- mapovani radku csv-->
        <property name="lineMapper">
            <bean class="org.springframework.batch.item.file.mapping.DefaultLineMapper">
                <!-- rozseknuti hodnot radku-->
                <property name="lineTokenizer" ref="batchLineDynamicTokenizer"/>
                <!-- do processoru budeme posilat DefaultFieldSet kde se budeme do mapy dotazovat na property -->
                <property name="fieldSetMapper">
                    <bean class="org.springframework.batch.item.file.mapping.PassThroughFieldSetMapper" />
                </property>
            </bean>
        </property>
    </bean>

</beans>